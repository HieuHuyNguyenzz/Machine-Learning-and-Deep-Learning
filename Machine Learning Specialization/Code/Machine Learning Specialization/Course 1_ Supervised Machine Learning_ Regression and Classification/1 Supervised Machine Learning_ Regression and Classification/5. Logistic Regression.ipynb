{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8s34eOXtfeVKKkjiE5WVt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DoNf8-auTrQ1"},"outputs":[],"source":["from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import time\n","import numpy as np\n","\n","# Tạo dữ liệu tổng hợp\n","X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=2, random_state=42)\n","\n","# Chuyển đổi sang DataFrame để dễ nhìn\n","df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n","df['target'] = y\n","\n","\n","x_train, x_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.3, random_state=42)\n","\n","x_train = x_train.values\n","y_train = y_train.values\n","x_test = x_test.values\n","y_test = y_test.values"]},{"cell_type":"code","source":["def loss_function(y, y_hat):\n","    \"\"\"Tính toán hàm mất mát (Binary Cross-Entropy)\"\"\"\n","    # Thêm một giá trị epsilon nhỏ để tránh log(0)\n","    epsilon = 1e-15\n","    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n","    return -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n","\n","def sigmoid_function(x):\n","    \"\"\"Hàm Sigmoid\"\"\"\n","    return 1 / (1 + np.exp(-x))\n","\n","def predict(x, weight, bias):\n","    \"\"\"Tính giá trị dự đoán y = wx + b\"\"\"\n","    weight = np.array(weight)\n","    return sigmoid_function(np.dot(x, weight) + bias)\n","\n","def compute_gradients(x, y, weight, bias):\n","    \"\"\"Tính gradient cho weight và bias\"\"\"\n","    predictions = predict(x, weight, bias)\n","    errors = predictions - y # Tính sai số\n","    dw = np.dot(x.T, errors) / len(y) # Gradient cho trọng số\n","    db = np.mean(errors) # Gradient cho độ lệch\n","    return dw, db\n","\n","def train_logistic_regression(x, y, learning_rate=0.1, epochs=1000):\n","    \"\"\"\n","    Huấn luyện mô hình hồi quy logistic sử dụng gradient descent\n","    Args:\n","        x: numpy array, biến độc lập\n","        y: numpy array, biến phụ thuộc\n","        learning_rate: tốc độ học\n","        epochs: số vòng lặp huấn luyện\n","    Returns:\n","        weight, bias: tham số mô hình đã huấn luyện\n","    \"\"\"\n","\n","    weight = np.ones(x.shape[1])\n","    bias = 0.0\n","\n","    # Kiểm tra đầu vào\n","    if len(x) != len(y):\n","        raise ValueError(\"Kích thước của x và y phải bằng nhau\")\n","    if len(x) == 0:\n","        raise ValueError(\"Dữ liệu đầu vào rỗng\")\n","\n","    # Huấn luyện mô hình\n","    for epoch in range(epochs):\n","        # Tính gradient\n","        dw, db = compute_gradients(x, y, weight, bias)\n","\n","        # Cập nhật tham số\n","        weight -= learning_rate * dw\n","        bias -= learning_rate * db\n","\n","        # In loss sau mỗi vài epoch để theo dõi\n","        if (epoch+1) % 100 == 0:\n","            loss = loss_function(y, predict(x, weight, bias))\n","            print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n","\n","\n","    return weight, bias\n","\n","try:\n","    st = time.time()\n","    final_weight, final_bias = train_logistic_regression(x_train, y_train) # Sử dụng .values để lấy numpy array từ Series\n","    ed = time.time()\n","    print(f\"\\nThời gian huấn luyện: {ed-st:.4f} giây\")\n","    # In trọng số và độ lệch\n","    print(f\"Bias cuối cùng: {final_bias:.4f}\")\n","    print(\"Weight cuối cùng:\")\n","    print(final_weight)\n","\n","\n","except Exception as e:\n","    print(f\"Lỗi trong quá trình huấn luyện: {str(e)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-d5HW8nZCKf","executionInfo":{"status":"ok","timestamp":1756048814869,"user_tz":-420,"elapsed":43,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"}},"outputId":"872582d9-283e-4505-ac29-939fba1025be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 100, Loss: 0.4660\n","Epoch 200, Loss: 0.4439\n","Epoch 300, Loss: 0.4421\n","Epoch 400, Loss: 0.4418\n","Epoch 500, Loss: 0.4417\n","Epoch 600, Loss: 0.4417\n","Epoch 700, Loss: 0.4417\n","Epoch 800, Loss: 0.4417\n","Epoch 900, Loss: 0.4417\n","Epoch 1000, Loss: 0.4417\n","\n","Thời gian huấn luyện: 0.0411 giây\n","Bias cuối cùng: 0.6661\n","Weight cuối cùng:\n","[ 0.49579574  0.35306445  0.02847362  1.73143567 -0.46153487 -0.0402296\n"," -0.53350539 -0.11800595  0.02896067  0.61709712]\n"]}]},{"cell_type":"code","source":["correct = 0\n","\n","logits = predict(x_test, final_weight, final_bias)\n","predictions = np.round(logits)\n","loss = loss_function(y_test, logits)\n","correct += (predictions == y_test).sum()\n","print(f\"Accuracy: {correct/len(y_test)*100}%\")\n","print(f\"Loss: {loss}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EVAEwUKelipw","executionInfo":{"status":"ok","timestamp":1756048801287,"user_tz":-420,"elapsed":10,"user":{"displayName":"EVILGODzz","userId":"10194818389342762833"}},"outputId":"78acd06d-ba14-4d03-d39a-47c14688a12f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 82.0%\n","Loss: 0.3842083716720332\n"]}]}]}